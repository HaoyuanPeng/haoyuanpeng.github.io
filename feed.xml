<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://haoyuanpeng.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://haoyuanpeng.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-02-07T07:30:43+00:00</updated><id>https://haoyuanpeng.github.io/feed.xml</id><title type="html">blank</title><subtitle>Senior Algorithm Engineer (NLP) at Learnable.AI, ex-Tencent, ex-FDU
</subtitle><entry><title type="html">山不在高，有仙则名</title><link href="https://haoyuanpeng.github.io/blog/2025/mountains/" rel="alternate" type="text/html" title="山不在高，有仙则名" /><published>2025-01-26T03:13:00+00:00</published><updated>2025-01-26T03:13:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2025/mountains</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2025/mountains/"><![CDATA[<h2 id="2000米以下">2000米以下</h2>
<ul>
  <li>正大体育馆台阶10m</li>
  <li>镇江北固山58.5m</li>
  <li><strong>佘山100.8m（海拔为2021年8月测绘，上海陆地最高峰，2025.01.25）</strong></li>
  <li>光华楼142m</li>
  <li>杭州西湖北高峰314m</li>
  <li>白云山摩星岭382m</li>
  <li>香山香炉峰575m</li>
  <li>括苍山1382.4m</li>
  <li>金紫尖1450.8m</li>
  <li>太子尖1558m（三尖穿越最高点）</li>
  <li>龙王山1587m（七尖穿越最高点）</li>
  <li><strong>必鲁图峰1611m（世界上海拔最高的沙峰，位于巴丹吉林沙漠，2015）</strong></li>
  <li>清凉峰1787.4m</li>
  <li><strong>黄山莲花峰1864.8m（安徽省最高峰）</strong></li>
  <li>武功山金顶1918.3m</li>
</ul>

<h2 id="2000米级">2000米级</h2>
<ul>
  <li>华山南峰2154.9m（2024.12.14）</li>
  <li><strong>灵山主峰2303m（北京最高峰，2025.01.11）</strong></li>
</ul>

<h2 id="3000米级">3000米级</h2>
<ul>
  <li>阿克布拉克达坂3820m（乌孙古道徒步最高点，2016.10）</li>
</ul>

<h2 id="4000米级">4000米级</h2>
<ul>
  <li>艾勒门特达坂4035m（夏特大环线徒步最高点，2024.10.04）</li>
  <li>擦缅垭口4650m（梅里南北线徒步最高点，2022）</li>
  <li>结斯沟穿山洞4880m（2024.12.07）</li>
  <li>耍拉垭口4970m（他念他翁徒步最高点，2020）</li>
</ul>

<h2 id="5000米级">5000米级</h2>
<ul>
  <li>四姑娘山大峰5025m（2024.10.27）</li>
  <li>错该达垭口5036m（亚丁大转山徒步最高点，2017）</li>
  <li><strong>乞力马扎罗山5895m（非洲最高峰，2020.01.30）</strong></li>
</ul>

<h2 id="6000米级">6000米级</h2>
<p>好日子还在后头呢</p>

<h2 id="7000m以上">7000m以上</h2>
<p>有梦想谁都了不起</p>]]></content><author><name></name></author><category term="fun stuff" /><summary type="html"><![CDATA[自己的登山记录]]></summary></entry><entry><title type="html">Linux机器上安装和配置nvidia-docker训练环境操作指南</title><link href="https://haoyuanpeng.github.io/blog/2024/install_ml_env/" rel="alternate" type="text/html" title="Linux机器上安装和配置nvidia-docker训练环境操作指南" /><published>2024-04-25T07:49:00+00:00</published><updated>2024-04-25T07:49:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2024/install_ml_env</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2024/install_ml_env/"><![CDATA[<p>Docker可以提供相互独立的训练环境，一方面可以实现不同用户训练环境的隔离，另一方面docker镜像可以被复用，以避免重复的环境搭建或者不同机器上环境不一致的问题。因此，在深度学习方向的开发机上使用docker是一个较好的实践。</p>

<p>本文介绍在完成NVIDIA-driver的安装后，安装nvidia-docker的过程。</p>

<h3 id="1-安装docker">1. 安装docker</h3>
<p>可以通过下方命令安装docker</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl https://get.docker.com | sh <span class="se">\</span>
  <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>systemctl <span class="nt">--now</span> <span class="nb">enable </span>docker
</code></pre></div></div>

<p>安装完成后，可以通过以下命令，确认docker已经安装成功。</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>docker version
</code></pre></div></div>

<h3 id="2-配置nvidia源">2. 配置nvidia源</h3>
<p>可以通过下方命令配置nvidia源</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">distribution</span><span class="o">=</span><span class="si">$(</span><span class="nb">.</span> /etc/os-release<span class="p">;</span><span class="nb">echo</span> <span class="nv">$ID$VERSION_ID</span><span class="si">)</span> <span class="se">\</span>
      <span class="o">&amp;&amp;</span> curl <span class="nt">-fsSL</span> https://nvidia.github.io/libnvidia-container/gpgkey | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span class="se">\</span>
      <span class="o">&amp;&amp;</span> curl <span class="nt">-s</span> <span class="nt">-L</span> https://nvidia.github.io/libnvidia-container/<span class="nv">$distribution</span>/libnvidia-container.list | <span class="se">\</span>
            <span class="nb">sed</span> <span class="s1">'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'</span> | <span class="se">\</span>
            <span class="nb">sudo tee</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list
</code></pre></div></div>

<h3 id="3-安装nvidia-docker">3. 安装nvidia-docker</h3>
<p>可以通过下方命令安装nvidia-docker</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> nvidia-container-toolkit
</code></pre></div></div>

<p>安装完成后，配置docker的运行环境，并重启docker</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nvidia-ctk runtime configure <span class="nt">--runtime</span><span class="o">=</span>docker
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div>

<h3 id="4-测试安装成功">4. 测试安装成功</h3>
<p>配置和重启完成后，可以通过下方命令新建容器并进行测试：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>docker run <span class="nt">--rm</span> <span class="nt">--runtime</span><span class="o">=</span>nvidia <span class="nt">--gpus</span> all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi
</code></pre></div></div>
<p>如果安装顺利完成，终端上将打印出机器上的显卡信息。</p>]]></content><author><name></name></author><category term="Machine Learning" /><summary type="html"><![CDATA[介绍如何基于nvidia-docker，在Linux机器上安装和配置用于训练深度学习模型的环境]]></summary></entry><entry><title type="html">高效计算幂取模和组合数取模</title><link href="https://haoyuanpeng.github.io/blog/2023/quick_pow_mod_conbination_mod/" rel="alternate" type="text/html" title="高效计算幂取模和组合数取模" /><published>2023-12-10T13:59:00+00:00</published><updated>2023-12-10T13:59:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2023/quick_pow_mod_conbination_mod</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2023/quick_pow_mod_conbination_mod/"><![CDATA[<p>给定正整数\(a, b, p\)，计算\(a^b\ mod\ p\)是RSA的加密和解密阶段均要使用的算法。
同时，在算法竞赛题目中，经常出现由于指数运算和排列组合运算的增长速度太快，一般的整形变量不足以表达结果的情况。此时，题目一般会要求将结果对一个特定的数取模。如果没有掌握相应的算法，在面对这种情况时，我们就容易遇到计算速度过慢导致超时，或者数值过大导致溢出的问题。</p>

<p>本文介绍高效计算\(a^b\ mod\ p\)和\(C(n, k)\ mod\ p\)并避免溢出的方法，希望能为读者带来帮助。</p>

<p><code class="language-plaintext highlighter-rouge">为方便起见，本文使用基于模运算符号$$mod$$的写法代替模同余的写法。例如，我们使用</code>\(17\ mod\ 4 = 1\)<code class="language-plaintext highlighter-rouge">来表示</code>\(17 \equiv 1 (mod 4)\)<code class="language-plaintext highlighter-rouge">。</code></p>

<h2 id="1-快速幂算法">1. 快速幂算法</h2>

<p>快速幂算法可以在\(O(log\ b)\)的时间复杂度内完成\(a^b\)的计算，相比较而言，直接计算则需要进行\(b\)次乘法。</p>

<p>快速幂算法利用了幂运算的法则：\(a^{b+c} = a^b * a^c\)。假设我们要计算\(3^{16}\)，注意到\(3^{16} = 3^{8 + 8} = 3^8 * 3^8\)。也就是说，我们只要算出\(3^{(16 // 2)}\)，就可以使用1次额外的乘法算出\(3^{16}\)，额外时间复杂度仅为\(O(1)\)。
当指数为奇数时，这个规律依然成立。假设我们要计算计算\(3^{17}\)，注意到\(3^{17} = 3^{8+8+1} = 3^8 * 3^8 * 3\)，在算出\(3^{(17 // 2)}\)之后，我们只需要使用2次额外的乘法即可算出\(3^{17}\)，额外时间复杂度也是\(O(1)\)。</p>

<p>从上述两个例子可见，我们可以将计算\(a^b\)的问题，在\(O(1)\)的时间内转换成计算\(a^{(b//2)}\)的问题，这也就意味着，我们可以在\(O(log\ b)\)的时间复杂度内将问题转换成常数时间复杂度的问题。这就是快速幂算法。</p>

<p>了解了快速幂算法的原理之后，我们可以用递归的方法实现快速幂算法。递归版本的python实现如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quick_pow</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">使用O(log b)时间复杂度计算a^b的算法（递归版本）。</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    
    <span class="n">h</span> <span class="o">=</span> <span class="nf">quick_pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">a</span>
    
    <span class="k">return</span> <span class="n">h</span> <span class="o">*</span> <span class="n">h</span>
</code></pre></div></div>

<p>我们也可以从另一个角度看待快速幂计算。设想我们要计算\(a^{13}\)，\(13\)的二进制表示是\(1101\)，即\(13=2^0*1
+2^1*0+2^2*1+2^3*1\)，，因此，\(a^{13}=a^{2^0*1}*a^{2^1*0}*a^{2^2*1}*a^{2^3*1}=
(a)^1*(a^2)^0*((a^2)^2)^1*(((a^2)^2)^2)^1\)。容易注意到，从第二项开始，每一项的底数是上一项的平方，而指数则与\(13\)的相应二进制位相同。
因此在一般情况下，要计算\(a^b\)，我们可以将结果初始化为\(1\)，乘数初始化为\(a\)。然后，我们从低到高遍历\(b\)的每一个二进制位，如果当前位的值是\(1\)，则将当前的乘数乘到结果上。然后不论当前位的值是\(0\)还是\(1\)，都将乘数更新为它的平方。</p>

<p>基于这个角度，我们可以实现循环版本的快速幂算法。python实现如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quick_pow</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">使用O(log b)时间复杂度计算a^b的算法（循环版本）。</span><span class="sh">"""</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">*=</span> <span class="n">a</span>
        <span class="n">a</span> <span class="o">*=</span> <span class="n">a</span>
        <span class="n">b</span> <span class="o">//=</span> <span class="mi">2</span>
        
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<h2 id="2-快速幂取模算法">2. 快速幂取模算法</h2>

<p>RSA需要将幂运算的结果对特定数字取模。同时，幂运算的增长速度很快，在算法竞赛中经常遇到指数运算的结果是一个天文数字的情况，此时，题目一般也会要求将结果对一个特定的数字取模。</p>

<p>基于快速幂算法，快速幂取模算法可以在\(O(log\ b)\)的时间复杂度内完成\(a^b\ mod\ p\)的计算。而且，当\(p\)较小时，无论\(a\)和\(b\)的值多大，都不会发生溢出。</p>

<p>快速幂取模算法的基本原理是取模运算的两个性质：\(a^b\ mod\ p = (a\ mod\ p)^b\ mod\ p\)以及\((a * b)\ mod\ p = (a\ mod\ p * b\ mod\ p)\ mod\ p\) 。我们依次证明这两个性质。</p>

<ol>
  <li>令\(m=a\ mod\ p\)，此时有\(a=kp+m\)。\(a^b = (kp + m)^b\)。根据二项式定理，\((kp + m)^b\)的展开式中除了最后一项\(m^b\)中\(p\)的次数为\(0\)外，其它的项均为\(p\)的倍数，因此，\(a^b\ mod\ p = (kp + m)^b\ mod\ p=m^b\ mod\ p\)，得证。</li>
  <li>令\(m_1=a\ mod\ p\)，\(m_2=b\ mod\ p\)，此时有\(a=k_1p+m_1, b=k_2p+m_2\)。\(a*b = k_1k_2p^2+k_1m_2p+k_2m_1p+m_1m_2\)。除\(m_1m_2\)外，其余各项均为\(p\)的倍数。因此，\((a*b)\ mod\ p=m_1m_2\ mod\ p\)，得证。</li>
</ol>

<p>基于这两个原理，我们在快速幂算法的基础上，首先对输入中的底数取模，并在每次乘法运算时，先对被乘数和乘数取模，并将结果相乘，再对结果取模。这就是快速幂取模算法。</p>

<p>快速幂取模算法的python实现如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quick_pow_mod</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">快速幂取模算法，计算a^b % p</span><span class="sh">"""</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">%</span> <span class="n">p</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">result</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span>
        <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span>
        <span class="n">b</span> <span class="o">//=</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>
<h2 id="3-组合数取模算法">3. 组合数取模算法</h2>
<p>在算法竞赛中，除了幂运算之外，组合数运算也可能产生较大的天文数字，需要对结果进行取模防止溢出。下面介绍：计算组合数\(C(n,k)\)对符合一定条件的正整数\(p\)取模的方法。</p>

<h3 id="31-组合数取模的基础算法">3.1 组合数取模的基础算法</h3>
<p>我们知道，\(C(n,k)=\frac{n!}{(n-k)!*k!}=\frac{n * (n - 1) * \cdots * (n - k + 1)}{1 * 2 * \cdots * k}\)。如果我们直接计算分子和分母的值，不仅速度很慢，而且当\(n\)和\(k\)较大时会发生溢出，一般的整型无法表示出分子和分母的准确数值。注意到分子和分母内部只包含乘法运算，我们可以使用模运算的乘法性质，分别求出分子和分母模\(p\)的余数。 此时，问题就被转换为：有两个数\(a\)和\(b\)，我们不知道\(a\)和\(b\)的具体数值，只知道它们各自模另一个整数\(p\)的余数，且\(a\)是\(b\)的倍数。求\(a/b\)模\(p\)的余数。</p>

<p>当\(p\)和\(b\)不互质时，仅凭\(a\ mod\ p\)和\(b\ mod\ p\)，无法唯一确定\((a/b)\ mod\ p\)的值，一个反例：\(a=14, b=2, p=6\)和\(a=8, b=2, p=6\)。而当\(p\)是质数，且\(p\)和\(a, b\)均互质时，这个问题是有高效的求解方法的。这也是算法竞赛中普遍选择大质数\(10^9+7\)作为\(p\)的值的原因。</p>

<p>注意，在下面的讨论中，我们默认\(p\)是质数，且\(p\)和\(a, b\)互质。由于组合数的分子\(a\)一定是分母\(b\)的倍数，不妨设\(a=kb，即a/b=k\)。</p>

<p>设想：如果我们能找到一个数\(b'\)，使得\(b * b'\ mod\ p =1\)，则有：</p>

\[\begin{aligned}
(a * b')\ mod\ p &amp;= (k * b * b')\ mod\ p \\ 
                 &amp;=(k\ mod\ p)*(b*b'\ mod\ p)\ mod\ p \\
                 &amp;= (k\ mod\ p * 1)\ mod\ p \\
                 &amp;= k\ mod\ p \\
                 &amp;= (a/b)\ mod\ p
\end{aligned}\]

<p>也就是说，如果我们能找到这样的\(b'\)，我们就能把模运算的除法问题转换为模运算的乘法问题！满足\(b * b'\ mod\ p=1\)的\(b'\)称为\(b\)的模\(p\)逆元。模逆元存在的充要条件是\(b\)和\(p\)互质，如果\(b\)和\(p\)的最大公约数不是1，则不论\(k\)取什么整数，\(kb\ mod\ p\)永远是最大公约数的倍数，不可能为1。</p>

<p>费马小定理可以帮助我们找到这样的\(b'\)。费马小定理指出：如果\(b\)是一个整数，\(p\)是一个质数，且\(b\)不是\(p\)的倍数，则有\(b^{p-1}\equiv p(mod\ 1)\)。例如5是一个整数，3是一个质数，5不是3的倍数，则\(5^{3-1}=25\)对3取模的结果是1。</p>

<p>由于\(b\)和\(b^{p-2}\)的乘积正好是\(b^{p-1}\)，而\(b^{p-1}\ mod\ p=1\)，这也就意味着，\(b\)的一个模\(p\)逆元就是\(b^{p-2}\)！由模运算的乘法性质可知，如果\(b^{p-2}\)是\(b\)的一个模\(p\)逆元，那\(b^{p-2}\ mod\ p\)也是\(b\)的一个模\(p\)逆元。尽管\(b\)和\(p\)的值可能很大，但我们可以使用快速幂取模算法，高效求出\(b^{p-2}\ mod\ p\)的值。</p>

<p>得到了\(b\)的模\(p\)逆元\(b'\)，我们就可以将\(a / b\ mod\ p\)转换为\(a * b'\ mod\ p\)，并利用模运算的乘法性质，在不发生溢出的情况下求出它的值了。</p>

<h3 id="32-组合数取模的高效算法lucas定理">3.2 组合数取模的高效算法：Lucas定理</h3>

<p>Lucas定理指出：</p>

\[C(n, k)\ mod\ p=C(n // p, k // p)\ * C(n\ mod\ p, k\ mod\ p)\ mod\ p\]

<p>当\(n\)和\(k\)很大时，\(n\ mod\ p\)和\(k\ mod\ p\)都小于\(p\)，而\(C(n // p, k // p)\)可以用Lucas定理进一步化简。因此，当问题规模很大时，Lucas定理可以帮助我们高效地化简问题。</p>]]></content><author><name></name></author><category term="Algorithm" /><summary type="html"><![CDATA[介绍如何高效计算a^b % p和C(n, k) % p并避免溢出]]></summary></entry><entry><title type="html">试答机器学习理论知识中的一些常见问题 (3 / 3)</title><link href="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_3_of_3/" rel="alternate" type="text/html" title="试答机器学习理论知识中的一些常见问题 (3 / 3)" /><published>2023-07-13T13:59:00+00:00</published><updated>2023-07-13T13:59:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2023/common_ml_problems_3_of_3</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_3_of_3/"><![CDATA[<p>有网友总结了机器学习理论知识中的一些常见问题，链接见：</p>

<ol>
  <li><a href="https://www.1point3acres.com/bbs/thread-713903-1-1.html">https://www.1point3acres.com/bbs/thread-713903-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714090-1-1.html">https://www.1point3acres.com/bbs/thread-714090-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714558-1-1.html">https://www.1point3acres.com/bbs/thread-714558-1-1.html</a></li>
</ol>

<p>现试答如下，欢迎批评指正。本文为第三部分，共三部分，每一部分与上述三个链接一一对应。</p>

<h4 id="写代码实现两层fully-connected网络"><strong>写代码实现两层fully connected网络</strong></h4>

<h4 id="手写cnn"><strong>手写CNN</strong></h4>

<h4 id="手写knn"><strong>手写KNN</strong></h4>

<h4 id="手写kmeans"><strong>手写Kmeans</strong></h4>

<h4 id="手写softmax的backpropagation"><strong>手写Softmax的backpropagation</strong></h4>

<h4 id="给你个lstm的结构让你计算how-many-parameters"><strong>给你个LSTM的结构让你计算how many parameters</strong></h4>

<h4 id="convolution-layer的output-size怎么算给出公式"><strong>Convolution layer的output size怎么算？给出公式</strong></h4>

<h4 id="训练好的模型在现实中不work问你可能的原因"><strong>训练好的模型在现实中不work，问你可能的原因</strong></h4>

<h4 id="loss趋于inf或nan的可能原因"><strong>Loss趋于inf或nan的可能原因</strong></h4>

<h4 id="生产和开发的时候data发生了一些shift应该如何detect和补救"><strong>生产和开发的时候，data发生了一些shift，应该如何detect和补救</strong></h4>

<h4 id="annotation有限的情况下怎么train-model"><strong>annotation有限的情况下怎么train model</strong></h4>

<h4 id="假设有个model要放production了但是发现online-one-important-feature-missing不能重新train-model你怎么办"><strong>假设有个model要放production了，但是发现online one important feature missing，不能重新train model，你怎么办？</strong></h4>

<h4 id="lstm的公式是什么"><strong>LSTM的公式是什么</strong></h4>

<h4 id="why-use-rnnlstm"><strong>Why use RNN/LSTM</strong></h4>

<h4 id="lstm比rnn好在哪"><strong>LSTM比RNN好在哪</strong></h4>

<h4 id="limitation-of-rnn"><strong>Limitation of RNN</strong></h4>

<h4 id="how-to-solve-gradient-vanishing-in-rnn"><strong>How to solve gradient vanishing in RNN?</strong></h4>

<h4 id="what-is-attention-why-attention"><strong>What is attention, why attention?</strong></h4>

<h4 id="language-model的原理n-gram-model"><strong>Language model的原理，N-gram model.</strong></h4>

<h4 id="what-is-cbow-and-skip-gram"><strong>What is CBOW and skip-gram?</strong></h4>

<h4 id="什么是word2vecloss-function是什么negative-sampling是什么"><strong>什么是word2vec，loss function是什么，negative sampling是什么</strong></h4>

<h4 id="maxpoolingconv-layer是什么为什么做pooling为什么用conv-layer什么是equivalent-to-translationinvariant-to-translation"><strong>maxpooling、conv layer是什么，为什么做pooling，为什么用conv layer？什么是equivalent-to-translation，invariant to translation</strong></h4>

<h4 id="1x1-filter"><strong>1x1 filter</strong></h4>

<h4 id="什么是skip-connection"><strong>什么是skip-connection</strong></h4>

<h4 id="what-is-bert-explain-the-model-structure"><strong>What is BERT, explain the model structure</strong></h4>

<h4 id="what-is-transformer-model-explain-the-model-structure"><strong>What is transformer model, explain the model structure.</strong></h4>

<h4 id="transformerbert比lstm好在哪"><strong>Transformer/BERT比LSTM好在哪</strong></h4>

<h4 id="difference-between-self-attention-and-traditional-attention-mechanism"><strong>Difference between self-attention and traditional attention mechanism.</strong></h4>

<h4 id="wide-and-deep"><strong>Wide and deep</strong></h4>

<h4 id="deepmask-unet等"><strong>Deepmask, UNet等</strong></h4>]]></content><author><name></name></author><category term="Machine Learning" /><summary type="html"><![CDATA[试答机器学习理论知识中的一些常见问题 (3 / 3)]]></summary></entry><entry><title type="html">试答机器学习理论知识中的一些常见问题 (2 / 3)</title><link href="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_2_of_3/" rel="alternate" type="text/html" title="试答机器学习理论知识中的一些常见问题 (2 / 3)" /><published>2023-07-10T10:00:00+00:00</published><updated>2023-07-10T10:00:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2023/common_ml_problems_2_of_3</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_2_of_3/"><![CDATA[<p>有网友总结了机器学习理论知识中的一些常见问题，链接见：</p>

<ol>
  <li><a href="https://www.1point3acres.com/bbs/thread-713903-1-1.html">https://www.1point3acres.com/bbs/thread-713903-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714090-1-1.html">https://www.1point3acres.com/bbs/thread-714090-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714558-1-1.html">https://www.1point3acres.com/bbs/thread-714558-1-1.html</a></li>
</ol>

<p>现试答如下，欢迎批评指正。本文为第二部分，共三部分，每一部分与上述三个链接一一对应。</p>

<h4 id="linear-regression的基本假设是什么"><strong>Linear Regression的基本假设是什么？</strong></h4>
<ol>
  <li><em>自变量X和y线性相关</em></li>
  <li><em>每个样本独立同分布</em></li>
  <li><em>Residual的均值为0</em></li>
  <li><em>Residual的方差在自变量的取值范围内是常数</em></li>
</ol>

<h4 id="what-will-happen-when-we-have-co-related-variables-how-to-solve"><strong>What will happen when we have co-related variables, how to solve?</strong></h4>
<p><em>模型会对输入x的微小变化过于敏感。如果两个自变量完全线性相关，则无法求解。可以通过Pearson系数或者VIF检测特征之间的相关性，并对特征进行合并或删除；或者使用Ridge、Lasso等正则化。</em></p>

<h4 id="explain-regression-coefficient"><strong>Explain regression coefficient</strong></h4>
<p><em>其它自变量保持不变时，某个自变量每变化1单位，则因变量的期望值的变化。</em></p>

<h4 id="whats-the-relationship-between-minimizing-squared-error-and-maximizing-the-likelihood"><strong>What’s the relationship between minimizing squared error and maximizing the likelihood?</strong></h4>
<p><em>当噪声满足高斯分布时，最小化MSE等价于最大化Likelihood。</em></p>

<h4 id="how-could-you-minimize-the-inter-correlation-between-variables-with-linear-regression"><strong>How could you minimize the inter-correlation between variables with Linear Regression?</strong></h4>
<p><em>Feature selection and removing highly-correlated features (or PCA), Regularization (Ridge or Lasso).</em></p>

<h4 id="if-the-relationship-between-y-and-x-is-not-linear-can-linear-regression-solve-that"><strong>If the relationship between y and x is not linear, can linear regression solve that?</strong></h4>
<p><em>传统线性回归不行，但可以在特征中加入其他项，如多项式项、对数项、交叉特征等，使得因变量与输入近似满足线性关系。</em></p>

<h4 id="why-use-interaction-variables"><strong>Why use interaction variables?</strong></h4>
<p><em>建模输入特征之间的相互作用，这也是非线性关系的一种。同时可以提供更丰富的可解释性。</em></p>

<h4 id="k-means-clustering-explain-the-algorithm-in-detail-whether-it-will-converge-global-or-local-optimums-how-to-stop"><strong>K-means clustering: explain the algorithm in detail, whether it will converge, global or local optimums, how to stop?</strong></h4>
<p><em>算法：</em></p>
<ol>
  <li><em>选择K个起始点（随机选择或者用Kmeans++选择），作为K个cluster的中心；</em></li>
  <li><em>每次迭代：计算每个样本点到各个cluster中心的距离，将其分到对应的中心；</em></li>
  <li><em>计算每个cluster的均值，作为新的中心；</em></li>
  <li><em>重复2～3步骤，直到每个聚类中心在两次迭代中保持不变，或达到最大迭代次数。</em></li>
</ol>

<p><em>K-means会收敛到局部最优，且要求每个cluster是凸的。</em></p>

<h4 id="em算法是什么"><strong>EM算法是什么？</strong></h4>
<p><em>EM算法是一种在概率模型包含不能观察的隐变量的情形下，对参数作最大似然估计的算法。算法通过两个步骤交替计算：</em></p>
<ol>
  <li><em>根据参数的假设值，给出未知变量的期望估计，并用在隐变量上；</em></li>
  <li><em>根据隐变量的估计值，给出当前参数的极大似然估计。</em></li>
</ol>

<h4 id="gmm是什么和k-means的关系"><strong>GMM是什么，和K-means的关系</strong></h4>
<p><em>GMM也是一种聚类算法，假设数据是由多个高斯分布组成的混合模型，每个高斯分布表示一个cluster。算法预测通过EM算法来估计模型参数，并得出每个样本点属于每个高斯分布的概率。GMM不要求每个聚类簇的形状是凸的。</em></p>

<p><em>K-means可以看作GMM的一个特例，即每个高斯分布的协方差矩阵是各向同性的。</em></p>

<h4 id="how-regression--classification-trees-split-nodes"><strong>How regression / classification trees split nodes?</strong></h4>
<ul>
  <li><em>Regression Tree: 一般以MSE为最小化的目标函数。对选定的特征，遍历其不同取值，计算相应的目标函数，并选择能够使其最小化的分裂点。</em></li>
  <li><em>Classification Tree: 遍历选定的特征的不同取值，计算相应的信息增益（即节点的熵 - 各分裂的子节点的熵的加权平均），选择能够使信息增益最大的分裂点。</em></li>
</ul>

<h4 id="how-to-prevent-over-fitting-in-dt"><strong>How to prevent over-fitting in DT?</strong></h4>
<ul>
  <li><em>剪枝：综合考虑树的节点个数和目标函数，对树进行剪枝；</em></li>
  <li><em>提前停止：根据叶节点的样本数量，或者信息增益的值，提前停止分裂；</em></li>
  <li><em>限制最大深度；</em></li>
  <li><em>ensemble。</em></li>
</ul>

<h4 id="how-to-do-regularization-in-dt"><strong>How to do regularization in DT?</strong></h4>
<p><em>同上</em></p>

<h4 id="difference-between-boosting-and-bagging"><strong>Difference between boosting and bagging</strong></h4>
<p><em>Boosting和Bagging都是将弱模型组合成强模型的方法。</em></p>
<ul>
  <li><em>Bagging方法中，弱模型并行地独立训练，并在最后预测时通过投票或者均值等方式集成。Bagging旨在降低模型的variance，解决过拟合问题，代表是随机森林。</em></li>
  <li><em>Boosting中，弱模型串行地组成pipeline。Boosting旨在降低模型的bias，提升模型效果，代表是GBDT。</em></li>
</ul>

<h4 id="gbdt和random-forest区别pros-and-cons"><strong>GBDT和random forest区别，pros and cons</strong></h4>
<p><em>GBDT是典型的Boosting方法，串行地训练N个DT，每个DT拟合之前的DT的残差。随机森林是典型的Bagging方法，通过bootstrapping对训练数据集进行不同的采样，并选择不同的特征子集，并行地同时训练N个DT。</em></p>

<h4 id="will-random-forest-reduce-bias-or-variance-why"><strong>Will random forest reduce bias or variance? Why?</strong></h4>
<p><em>variance。因为随机森林中的每个模型仅使用部分的特征，且通过bootstrap的方法对训练样本进行采样，每个DT的训练样本各不相同。最后，通过投票集成也可以降低variance。</em></p>

<h4 id="和discriminative模型相比generative模型更容易over-fitting还是under-fitting"><strong>和Discriminative模型相比，Generative模型更容易over-fitting还是under-fitting?</strong></h4>
<p><em>在相同的数据条件下，判别模型只需要建模条件分布，而不需要对完整的数据分布进行建模；同时生成模型为了更准确地拟合训练数据中的分布，更有可能关注到数据的内在结构和噪声，而判别模型只需要关注类别边界。因此，生成模型更容易过拟合。</em></p>

<h4 id="naive-bayes的原理基础假设是什么"><strong>Naive-Bayes的原理，基础假设是什么？</strong></h4>
<p><em>Naive Bayes的基础假设是特征之间的条件独立，即在给定类别的条件下，各输入特征之间相互独立。</em></p>

<p><em>Naive Bayes的原理是，利用Bayes定理，基于P(类别）、P(特征)、P(特征|类别)，计算出P(类别|特征)。</em></p>

<h4 id="ldaqda是什么基础假设是什么"><strong>LDA/QDA是什么，基础假设是什么？</strong></h4>
<p><em>LDA试图找到一个低维空间，使得将不同类别的样本投影到这个空间上时，同类样本之间的距离小，不同类样本之间的距离大，从而可以使用线性分类器进行分类。LDA的基础假设是不同类别的样本的特征的协方差矩阵相同。</em></p>

<p><em>QDA暂缺。</em></p>

<h4 id="logistic-regression和svm的差别"><strong>Logistic Regression和SVM的差别</strong></h4>
<p><em>Logistic Regression采用Log Loss，即最小化Negative Log Likelihood，而SVM采用Hinge Loss。</em></p>

<h4 id="explain-svm如何引入非线性"><strong>Explain SVM，如何引入非线性</strong></h4>
<p><em>SVM是一种二分类算法，原理是在特征空间中找到一个最优超平面，将不同类别的样本分隔开来。最优的超平面指的是离最近的样本点最远的超平面，距离超平面最近的样本点称为支持向量。</em></p>

<p><em>SVM可以通过特征的非线性变换以及引入组合特征来将非线性分类问题转换为线性分类问题，也可以通过核函数将输入特征映射到高维空间来实现非线性划分。</em></p>

<h4 id="explain-pca"><strong>Explain PCA</strong></h4>
<p><em>PCA是一种特征降维的方法。PCA首先对数据进行标准化处理，然后计算协方差矩阵，并对协方差矩阵进行特征值分解，选择具有最大值的前k个特征向量。然后将原始数据通过选取的k个特征向量进行线性变换来映射到k维的低维空间。</em></p>

<p><em>PCA假设数据的方差集中在少数的k个维度上，要评估数据是否满足这个假设。</em></p>

<h4 id="explain-kernel-methods-why-to-use"><strong>Explain kernel methods, why to use.</strong></h4>
<p><em>核函数将输入特征映射到高维空间，从而解决非线性划分的问题。同时，核函数可以计算样本在高维空间的内积，而无需显式计算样本在高维空间的表示。</em></p>

<p><em>常见的核函数包括多项式核函数、RBF核函数、Sigmoid核函数等。</em></p>

<h4 id="怎么把svm的output按照概率输出"><strong>怎么把SVM的output按照概率输出？</strong></h4>

<p><em>暂缺</em></p>

<h4 id="explain-knn"><strong>Explain KNN</strong></h4>
<p><em>KNN是一种有监督分类算法，通过训练样本中，与输入样本距离最近的k个样本的标签，来对输入样本的标签进行预测。</em></p>

<h4 id="怎么处理imbalanced-data"><strong>怎么处理imbalanced data</strong></h4>
<ol>
  <li><em>对类别较多的样本进行降采样，对类别较少的样本进行过采样；</em></li>
  <li><em>为不同类别在损失函数中设置不同的权重；</em></li>
  <li><em>将任务转换为异常检测任务；</em></li>
  <li><em>使用集成学习来降低模型对多数样本的过拟合。</em></li>
  <li><em>使用合适的评价指标，在不平衡数据上对模型进行评价，如precision、recall、F值等。</em></li>
</ol>

<h4 id="high-dim-classification有什么问题如何处理"><strong>High-dim classification有什么问题，如何处理？</strong></h4>
<ol>
  <li><em>维度灾难问题：特征选择或降维；</em></li>
  <li><em>过拟合问题：正则化、交叉验证、ensemble；</em></li>
  <li><em>计算复杂度问题：特征选择或降维。</em></li>
</ol>

<h4 id="missing-data如何处理"><strong>Missing data如何处理</strong></h4>
<ol>
  <li><em>删除有缺失值的样本，用剩下的样本训练模型作为baseline，并评价缺失值填充的效果。</em></li>
  <li><em>特殊值填充、平均值填充、众数填充、基于距离最近的k个其他样本赋值、插值、模型预测。</em></li>
</ol>

<h4 id="how-to-do-feature-selection"><strong>How to do feature selection</strong></h4>
<ol>
  <li><em>Pearson相关系数；</em></li>
  <li><em>卡方特征选择；</em></li>
  <li><em>方差选择；</em></li>
  <li><em>基于互信息选择；</em></li>
  <li><em>基于L1正则产生稀疏特征；</em></li>
  <li><em>使用PCA或者LDA进行降维。</em></li>
</ol>

<h4 id="how-to-capture-feature-interaction"><strong>How to capture feature interaction?</strong></h4>
<ol>
  <li><em>原始特征的多项式扩展，如x1 * x2, x1^2等；</em></li>
  <li><em>交叉特征：如gender_age；</em></li>
  <li><em>通过深度学习模型自动学习。</em></li>
</ol>]]></content><author><name></name></author><category term="Machine Learning" /><summary type="html"><![CDATA[试答机器学习理论知识中的一些常见问题 (2 / 3)]]></summary></entry><entry><title type="html">试答机器学习理论知识中的一些常见问题 (1 / 3)</title><link href="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_1_of_3/" rel="alternate" type="text/html" title="试答机器学习理论知识中的一些常见问题 (1 / 3)" /><published>2023-07-02T08:51:00+00:00</published><updated>2023-07-02T08:51:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2023/common_ml_problems_1_of_3</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2023/common_ml_problems_1_of_3/"><![CDATA[<p>有网友总结了机器学习理论知识中的一些常见问题，链接见：</p>

<ol>
  <li><a href="https://www.1point3acres.com/bbs/thread-713903-1-1.html">https://www.1point3acres.com/bbs/thread-713903-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714090-1-1.html">https://www.1point3acres.com/bbs/thread-714090-1-1.html</a></li>
  <li><a href="https://www.1point3acres.com/bbs/thread-714558-1-1.html">https://www.1point3acres.com/bbs/thread-714558-1-1.html</a></li>
</ol>

<p>现试答如下，欢迎批评指正。本文为第一部分，共三部分，每一部分与上述三个链接一一对应。</p>

<h4 id="overfitting--underfiting是指的什么"><strong>overfitting / underfiting是指的什么</strong></h4>

<p><em>overfitting指的是模型过度拟合了训练集的样本，包括其中的噪声，表现为训练集的Loss很低，但验证集的Loss较高；
underfitting指的是因为参数量、特征选择、训练进度等原因，未能充分学习训练集中样本的特征，表现为训练集和验证集的loss都较高。</em></p>

<h4 id="biasvariance-trade-off是指的什么"><strong>bias/variance trade off是指的什么</strong></h4>

<p><em>bias指的是模型的预测结果与Ground Truth之差的期望，variance指的是同一个模型在不同的训练数据下，对同一个样本的预测结果的方差。一般而言，随着模型复杂度和特征复杂度的增加，bias会下降，而variance会上升，因此需要找到一个合适的复杂度，使得bias^2 + variance最小。</em></p>

<h4 id="过拟合一般有哪些预防手段"><strong>过拟合一般有哪些预防手段</strong></h4>

<p><em>数据方面：增加训练数据量；对训练数据进行数据增强；减少训练数据的特征数量；</em></p>

<p><em>模型和训练策略方面：使用单独的验证集或使用交叉验证来观察是否发生了过拟合；使用Regularization控制模型参数的scale；使用Early Stopping控制训练进度；使用Ensemble降低Variance。</em></p>

<h4 id="generative和discriminative的区别"><strong>Generative和Discriminative的区别</strong></h4>

<p><em>对于输入特征x和样本y，generative model学习P(x,y)，而discriminative model学习P(y|x)</em></p>

<h4 id="give-a-set-of-ground-truths-and-2-models-how-do-you-be-confident-that-one-model-is-better-than-another"><strong>Give a set of ground truths and 2 models, how do you be confident that one model is better than another?</strong></h4>

<p><em>确定合适的metric，并使用k-fold cross validation对模型效果进行验证。</em></p>

<h4 id="l1-vs-l2-which-one-is-which-and-difference"><strong>L1 vs L2, which one is which and difference</strong></h4>

<p><em>L1 Norm是将所有参数的绝对值之和作为正则项，L2 Norm是将所有参数的平方和作为正则项。L1对outlier更Robust，且有稀疏性；L2计算起来更方便，且有唯一解。</em></p>

<h4 id="lasso--ridge-的解释prior分别是什么"><strong>Lasso / Ridge 的解释（Prior分别是什么）</strong></h4>

<p><em>Lasso是指加入系数的L1 Norm作为正则项，而Ridge是加入系数的L2 Norm作为正则项。L1的prior是认为参数w的先验概率满足拉普拉斯分布，而L2的prior是认为参数w的先验满足高斯分布。</em></p>

<h4 id="lasso--ridge-的推导"><strong>Lasso / Ridge 的推导</strong></h4>

<p><em>Lasso:</em></p>

\[\begin{aligned}
L(w)=\|Xw - Y\|^2 + \alpha \sum \|w_j\|  \\
\frac{\partial L(w)}{\partial w} = 2X^TXw - 2X^TY + \alpha C \\
w = (X^TX)^{-1}(X^TY-\frac{\alpha}{2}C)

\end{aligned}\]

<p><em>Ridge:</em></p>

\[\begin{aligned}
L(w)=\|Xw - Y\|^2 + \alpha \sum \|w_j\|^2 \\
\frac{\partial L(w)}{\partial w} = 2X^TXw - 2X^TY + 2\alpha w \\
(X^X+\alpha I)w = X^TY \\
w = (X^TX + \alpha I)^{-1}X^TY

\end{aligned}\]

<h4 id="为什么l1比l2稀疏"><strong>为什么L1比L2稀疏？</strong></h4>

<p><em>定义损失函数与某个权重参数x的函数为L(x)。</em>
<em>当施加L1 Norm的时候，\(f(x)=L(x)+C|x|\)，则\(f'(0-)=L'(0)-C，f'(0+)=L'(0)+C\)，要使0成为f的极值点，则需要\(f'(0-)*f'(0+)&lt;0\)，解得\(C&gt;|L'(0)|\)。也就是说，只要正则项的系数C大于Loss function在参数x为0的时候的导数的绝对值，则x的最优解就是0。</em></p>

<p><em>当施加L2 Norm的时候，\(f(x)=L(x)+Cx^2\)，则\(f'(0)=L'(0)\)，要使x=0为极值点，则需要\(L'(0)=0\)，条件很难满足。</em></p>

<h4 id="为什么regularization-works"><strong>为什么regularization works？</strong></h4>

<p><em>因为过拟合的曲线考虑到了训练集上每个样本点的噪声，因此会发生“突变”的现象，即斜率会变得较大。对线性模型来说，斜率其实就是参数的值，推广到多维复杂模型，参数的值越大，越容易发生突变。而加入正则项之后，Loss function与参数值相关，优化器要降低loss，就要降低参数的值，从而降低突变的可能性，因此可以抑制过拟合现象。</em></p>

<p><em>对于Lasso而言，因为将权重惩罚到了0，相当于降低了模型使用的特征数量即复杂度。</em></p>

<h4 id="为什么regularization用l1l2而不用l3l4"><strong>为什么regularization用L1、L2，而不用L3、L4？</strong></h4>

<p><em>因为L1和L2的效果已经得到了充分验证，且L1能产生稀疏特征。相比较而言，L3和L4的计算复杂度更大，效果也没有充分验证。</em></p>

<h4 id="precision-and-recall-trade-off"><strong>Precision and Recall, trade-off</strong></h4>

<p><em>Precision表示True Positive占所有预测的Positive的比例，衡量模型的产生False Positive的程度。
Recall表示True Positive占所有正样本的比例，衡量模型产生False Negative的程度。</em></p>

<p><em>可用通过F Score，基于Precision和Recall对实际任务的权重，对Precision和Recall进行Trade Off.</em></p>

<h4 id="label不平衡时用什么metric"><strong>label不平衡时用什么metric</strong></h4>

<p><em>可以用各个类别，或者关注的正样本类别（二分类）的P、R、F1作为metric，在二分类场景下，也可以用ROC-AUC作为metric。</em></p>

<h4 id="分类问题该用什么metricand-why"><strong>分类问题该用什么metric，and why？</strong></h4>

<p><em>Accuracy、Top-K acc、P、R、F、混淆矩阵，P-R Curve、ROC-AUC</em></p>

<h4 id="confusion-matrix"><strong>confusion matrix</strong></h4>

<p><em>一个矩阵，matrix[i][j]表示实际为第i类，预测为第j类的样本数量。</em></p>

<h4 id="auc的解释the-probability-of-ranking-a-randomly-selected-positive-sample-higher-"><strong>AUC的解释（The probability of ranking a randomly selected positive sample higher …)</strong></h4>

<p><em>ROC曲线是二分类模型在不同分类阈值下的效果曲线，横坐标为FPR，即假阳率；纵坐标为召回率。FPR=0时，Recall=0，FPR=1时，Recall=1。因此ROC曲线
连接了(0,0)和(1, 1)两点。ROC-AUC是指曲线下的面积，面积越大，表示模型在相同的FPR下，Recall越高，因此模型的效果越好。同时，ROC-AUC也正好等于
随机挑选一个正样本和一个负样本，正样本的得分（即预测为正样本的概率）比负样本高的概率。</em></p>

<p><em>ROC-AUC的概率意义的证明暂缺。</em></p>

<h4 id="true-positive-rate-false-positive-rate-roc"><strong>true positive rate, false positive rate, ROC</strong></h4>

<p><em>TPR = Recall，FPR = FP / (FP + TN)；ROC为横坐标为FPR，纵坐标为TPR的曲线，可以衡量模型的效果。ROC仅对排序敏感，而对具体的预测概率值不敏感。
两个模型可能ROC-AUC相同，但在不同的FPR区间表现的效果差异较大（MisLeading)。同时，当用户对FPR和TPR有不同的重视程度时，这个指标无法满足。</em></p>

<h4 id="log-loss是什么什么时候用log-loss"><strong>Log-loss是什么，什么时候用Log-loss</strong></h4>

<p><em>Log-loss就是Likelihood的负对数，即\(-logP(Y_{gt}\vert X)\)，在分类场景，尤其是关注类别置信度的分类场景时适用。</em></p>

<h4 id="场景相关的问题比如ranking-design的时候用什么metric推荐的时候用什么等"><strong>场景相关的问题，比如ranking design的时候用什么metric，推荐的时候用什么等？</strong></h4>

<p><em>排序任务的指标：</em></p>
<ul>
  <li><em>MAP：将排序后，所有relevant position处的precision进行平均</em></li>
  <li><em>MRR：将Top-1所在rank的倒数作平均</em></li>
  <li><em>NDCG：DCG就是每个位置的相关性，除以相应位置的对数（排序越往后，重要性越低）。NDCG就是DCG除以DCG的理论最大值。</em></li>
</ul>

<h4 id="用mse做loss的logistic-regression是convex-problem吗"><strong>用MSE做loss的Logistic Regression是convex problem吗？</strong></h4>

<p><em>不是。证明如下：</em></p>

\[\begin{aligned}
令f(x) = (y - \hat y)^2, 其中\hat y = \frac{1}{1 + e^{-\theta x}} \\
则有\frac{\partial f}{\partial \theta} = \frac{\partial f}{\partial \hat y} \frac{\partial \hat y}{\partial \theta} \\
=-2(y-\hat y)\hat y(1-\hat y)x \\
=-2(y\hat y - y\hat y ^ 2 - \hat y^2 + \hat y^3)x

\frac{\partial^2 f}{\partial \theta^2} \\
= -2(y\hat y - y\hat y ^ 2 - \hat y^2 + \hat y^3)x \cdot {\hat y (1 - \hat y)x} \\  
\\ \because \hat y \in (0, 1)
\\ \therefore x^2\hat y (1- \hat y) &gt; 0
\end{aligned}\]

<p><em>在\(y=0\)和\(y=1\)时分别求上式&gt;0的\(\hat y\)的取值范围，可得上式有正有负，所以是non convex的。</em></p>

<h4 id="解释并写出mse的公式什么时候用到mse"><strong>解释并写出MSE的公式，什么时候用到MSE？</strong></h4>
<p><em>MSE就是Mean Squared Error，即每个样本的预测值与真实值之差的平方的平均值。在回归任务中，假设真实值和观测值的误差服从高斯分布，则会用到MSE，如果误差服从拉普拉斯分布，则用MAE。</em></p>

<h4 id="linear-regression最小二乘法和mse的关系"><strong>Linear Regression最小二乘法和MSE的关系</strong></h4>
<p><em>最小二乘法就是最小化MSE的一种求解方法。</em></p>

<h4 id="什么是relative-entropy--cross-entropy以及kl-divergence他们的intuition"><strong>什么是relative entropy / cross entropy，以及KL Divergence，他们的intuition</strong></h4>
<p><em>Relative entropy就是KL Divergence，衡量两个概率分布的差异。</em>\(D_{KL}(P \| Q) = \sum_i P(i)ln\frac{P(i)}{Q(i)}\)</p>

<p><em>KL Divergence和交叉熵：\(D_{KL}(P \| Q) = -S(P) + H(P, Q)\)</em></p>

<h4 id="logistic-regression的loss是什么"><strong>Logistic Regression的Loss是什么？</strong></h4>
<p><em>Negative Log Likelihood.</em></p>

<h4 id="logistic-regression的loss推导"><strong>Logistic Regression的Loss推导</strong></h4>

\[\begin{aligned}
令\beta表示模型参数，x_i表示输入，y_i表示正确结果，\delta = \frac{1}{1 + e^{-\beta^Tx_i}}，则有 \\
p(y_i\vert x_i, \beta) = \delta^{y_i}(1-\delta)^{1-y_i} \\

NLL = -\sum ln(\delta^{y_i}(1-\delta)^{1-y_i}) \\

\frac{\partial NLL}{\partial \beta} = -\sum (y_i \cdot \frac{1}{\delta} \cdot \delta (1-\delta)x_i + (1-y_i) \cdot \frac{1}{1-\delta} \cdot -\delta (1- \delta)x_i )\\
= -\sum(\delta - y_i)x_i \\
\end{aligned}\]

<h4 id="svm的loss是什么"><strong>SVM的Loss是什么</strong></h4>
<p><em>Hinge Loss. \(L = max(0, 1 - ty)\)，其中t为目标值，取值为1或-1，y为预测值。不鼓励模型过度自信。</em></p>

<h4 id="multiclass-logistic-regression然后问了一个为什么用cross-entropy做loss-function"><strong>Multiclass Logistic Regression，然后问了一个为什么用cross entropy做Loss function。</strong></h4>
<p><em>K个1 vs. others分类器。在Logistics Regression中，优化cross entropy等于优化NLL.</em></p>

<h4 id="decision-tree-split-nodes时的优化目标是啥"><strong>Decision Tree split nodes时的优化目标是啥</strong></h4>
<p><em>信息增益。</em></p>

<h4 id="dnn为什么要有bias-termbias-term的intuition是什么"><strong>DNN为什么要有bias term，bias term的intuition是什么？</strong></h4>
<p><em>如果没有bias，则分类超平面则必须经过原点。Bias可以对分类超平面进行平移。</em></p>

<h4 id="什么是back-propagation"><strong>什么是Back propagation?</strong></h4>
<p><em>Back Propagation是指误差反向传播，是一种对神经网络进行梯度下降的算法，即用链式法则计算损失函数对每层权重的梯度，并据此来更新权重。</em></p>

<h4 id="梯度消失和梯度爆炸是什么怎么解决">梯度消失和梯度爆炸是什么？怎么解决？</h4>
<p><em>梯度消失和梯度爆炸是训练深层神经网络或者Vanilla RNN时存在的问题。使用Back Propagation算法进行链式求导时，对于靠近输入端的权重，其梯度是每一层权重以及每一层的激活函数的梯度的乘积数，如果梯度较小（如
使用了不合适的激活函数，如Sigmoid，其导数的取值范围是(0, 0.25))，会导致梯度的值逐层减小，最终消失。梯度消失表现为随着模型训练，loss下降很慢，尤其是离输出层近的权重改变幅度大，离输出层远的权重改变幅度小甚至为0。反之，如果梯度较大，会导致
梯度逐层指数增加，产生梯度爆炸，表现为loss值和模型权重剧烈变化，甚至变成NaN。</em></p>

<p><em>解决梯度消失问题的方法主要有：更换激活函数为Relu或其变种、增加Residual Connection和使用Batch Norm。</em></p>

<p><em>解决梯度爆炸问题的方法主要有：梯度裁剪、加入正则项、更好的初始化策略等。</em></p>

<h4 id="神经网络初始化能不能把weights都设置成0"><strong>神经网络初始化能不能把weights都设置成0？</strong></h4>
<p><em>不能，也不能设为相同的值，否则会导致每一层Hidden Layer中各个节点的值完全相同，根据反向传播算法，每个参数更新的值也相同，无法正常学习特征。</em></p>

<h4 id="dnn和logistic-regression区别"><strong>DNN和Logistic Regression区别</strong></h4>
<p><em>可以把Logistic Regression看成一个只有一层的神经网络，只能学习特征各自的权重（有可解释性），并将特征的加权和通过sigmoid函数进行二分类。</em></p>

<h4 id="你为什么觉得dnn拟合能力比logistic-regression强"><strong>你为什么觉得DNN拟合能力比Logistic Regression强？</strong></h4>
<p><em>因为DNN可以学习特征的组合关系，而Logistic Regression只能学习特征各自的权重。</em></p>

<h4 id="how-to-do-hyper-parameter-tuning-in-dl"><strong>How to do hyper-parameter tuning in DL?</strong></h4>
<p><em>可以通过Grid Search的方法进行，也有一些AutoML的方法，如贝叶斯优化。</em></p>

<h4 id="deep-learning有哪些预防over-fitting的方法"><strong>Deep Learning有哪些预防over fitting的方法？</strong></h4>
<p><em>Weight Decay、Dropout、Pretrain + Finetune、Early Stopping、数据增强等。</em></p>

<h4 id="什么是dropoutwhy-it-works-dropout的流程是什么"><strong>什么是Dropout？Why it works? Dropout的流程是什么？</strong></h4>
<p><em>Dropout是一种在训练深度神经网络时防止过拟合的方法。在训练阶段的前向过程中，通过将hidden layer的值直接设为0的方式，忽略一部分的隐藏层神经元。这部分神经元不产生梯度传播。在推理阶段则保留全部的隐藏层神经元，并将神经元的输出进行Rescale。</em></p>

<p><em>Dropout有效果，一方面是在训练的时候迫使每个神经元的取值不依赖于任何一个特定的其它神经元，降低了网络的复杂度。如果把不同的dropout看成是不同的子网络，也体现一种集成学习的思想。</em></p>

<h4 id="什么是batch-normwhy-it-works-bn的流程是什么"><strong>什么是Batch Norm？Why it works? BN的流程是什么？</strong></h4>
<p><em>BN是一种用于神经网络的Regularization技术，目的是解决神经网络中的Internal covariate shift的问题，即随着神经网络的训练，权重的分布发生变化，导致输出给下一层的特征的分布发生变化，而这个变化需要下一层的网络权重额外学习。</em></p>

<p><em>BN包括两个阶段，归一化和scale and shift。它学习两个参数，\(\gamma\)和\(\beta\)。在训练阶段，对每个mini batch的输入，BN层计算其均值和标准差，并将其归一化到标准正态分布。然后通过使用学习到的\(\gamma\)和\(\beta\)进行缩放和平移。</em></p>

<p><em>在推理阶段，因为只有一个样本的输入，无法计算均值和方差，所以在训练的时候会同时记录训练样本的均值和方差。训练样本的均值和方差一般通过滑动平均的方式来更新。</em></p>

<h4 id="common-activation-functions是什么以及每个的优缺点"><strong>common activation functions是什么以及每个的优缺点</strong></h4>
<p><em>Sigmoid：可以将输入映射到0和1之间的连续输出，可以解释为概率并用于二分类问题。缺点：容易造成梯度消失，且输出不是以0为中心。</em></p>

<p><em>tanh：将输入映射到-1到1之间，产生以0为中心的输出。缺点：计算开销大，且输入远离0的时候，梯度也接近0，容易造成梯度消失。</em></p>

<p><em>ReLU：将负的输入映射为0，正的输入保持不变，简单有效，计算速度快，产生稀疏特征。缺点：输入为负的神经元梯度为0。</em></p>

<p><em>Leaky ReLU: ReLU的改进，输入为负的时候引入一个小的斜率，可以允许梯度的传播。</em></p>

<p><em>还有一些基于ReLU的改进，如引入可学习的参数来控制斜率等。</em></p>

<h4 id="为什么需要non-linear-activation-functions"><strong>为什么需要Non-Linear activation functions？</strong></h4>
<p><em>如果没有非线性激活函数，则DNN退化成对输入特征的线性变换，不能产生非线性的决策边界。</em></p>

<h4 id="different-optimizers-sgd-rmsprop-momentum-adagrad-adam的区别"><strong>Different Optimizers （SGD, RMSProp, Momentum, Adagrad, Adam）的区别</strong></h4>
<p><em>SGD是基本的梯度下降算法，每次迭代通过当前样本的梯度更新模型参数。\(\theta_{t+1} = \theta_t - \eta \nabla J(\theta)\) 。SGD（包括Mini-Batch GD)有两个问题：1.容易陷在局部最小值（saddle point或者plateau)。 2.学习率始终固定。动量方法应对问题1，而自适应学习率方法应对问题2。</em></p>

<p><em>Momentum在SGD的基础上增加了动量概念，即在参数更新时，既利用当前Batch的梯度，也保留原有的更新方向，即</em></p>

\[\begin{aligned}
m_{t+1}=\mu \cdot m_t + \eta \nabla J(\theta); \\
\theta_{t+1} = \theta_t - m_{t+1}
\end{aligned}\]

<p><em>Nestrov accelerate gradient是Momentum的变种，具体做法是计算梯度时，先在\(\theta\)上减去上一时刻的动量</em></p>

\[\begin{aligned}
m_{t+1}=\mu \cdot m_t + \eta \nabla J(\theta - \mu \cdot m_t); \\
\theta_{t+1} = \theta_t - m_{t+1}
\end{aligned}\]

<p><em>Adagrad是一种自适应学习率算法，目的是对学习率进行动态调整。在更新梯度时，对每个参数，将全局学习率除以一个基于梯度累计范数的约束项，这样前期的梯度更新量较大，后期梯度更新量较小。频繁更新的参数梯度更新量较小，稀疏更新的参数梯度更新量较大。</em></p>

\[\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, i} + \epsilon}} \cdot \nabla J(\theta_{t,i})\]

<p><em>Adagrad的问题是到了训练中后期，因为约束项分母过大，导致参数更新量太小，无法学习。RMSProp将不断累计的梯度更新量之和改为RMS形式的梯度移动平均。</em></p>

\[\begin{aligned}
E[g^2]_t = \rho \cdot E[g^2]_{t-1} + (1-\rho) \cdot g_t^2 \\
\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot \nabla J(\theta_{t})  = \theta_t - \frac{\eta}{RMS[g]_t}\cdot \nabla J(\theta_{t}) 
\end{aligned}\]

<p><em>Adadelta将RMSProp中的固定学习率参数\(\eta\)转换为上一时刻的\(\Delta \theta\)的RMS，不需要指定固定的学习率。(Pytorch中的adadelta优化器有学习率参数lr，但它是可选参数，默认值是1.0，相当于对自适应梯度再做一次scale）。</em></p>

\[\theta_{t+1} = \theta_t - \frac{RMS[\Delta(\theta)]_{t-1}}{RMS[g]_t}\cdot \nabla J(\theta_{t})\]

<p><em>Adam优化器可以看作是动量和自适应学习率的结合，类似RMSProp + Momentum。Adam保留过去梯度的指数衰减平均值，也保留梯度的平方的指数衰减平均值：</em></p>

\[\begin{aligned}
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t = \beta_2 v_{t-1} + (1-\beta_2)g^2_t \\
\hat m_t = \frac{m_t}{1-\beta^t_1} \\
\hat v_t = \frac{v_t}{1-\beta^t_2} \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat v_t + \epsilon}} \cdot \hat m_t
\end{aligned}\]

<p><em>目前主流的优化器是AdamW，即增加了WeightDecay的Adam优化器。AdamW优化器将参数权重的decay从梯度计算中分离出去，单独基于L2正则化计算权重衰减。</em></p>

<h4 id="batch和sgd的优缺点batch-size的影响"><strong>Batch和SGD的优缺点，Batch size的影响</strong></h4>
<p><em>Batch GD（区别于Mini Batch GD）是一次性迭代所有的训练样本，而SGD是每次迭代一个训练样本，Mini-Batch GD是两者的折中，通过超参数Batch Size控制每次迭代的样本数量。</em></p>

<p><em>BGD优点是较为稳定，缺点是每次迭代完整的数据集，速度较慢，且无法应用到大数据集上；</em></p>

<p><em>SGD的优点是计算成本低，内存需求小，缺点是受噪声影响大，更新方向的方差较大，收敛过程不稳定且对学习率敏感。</em></p>

<p><em>Mini Batch作为两者的折中，是主流的方案，可以通过一个mini-batch中的样本近似整体梯度，减少更新方向的方差，更新更稳定，也可以通过并行化来提升计算效率。</em></p>

<p><em>Batch Size的影响主要是内存/显存的占用量，同时，batch size发生变化时，学习率也要相应调整。</em></p>

<h4 id="learning-rate过大和过小对模型的影响"><strong>learning rate过大和过小对模型的影响</strong></h4>
<p><em>学习率设置过大时，容易出现梯度爆炸的问题，也容易产生Loss的震荡。学习率设置过小时，会使得模型的收敛速度太慢，且容易陷入局部最小值无法跳出。主流的做法会对学习率进行warmup和衰减。</em></p>

<h4 id="problem-of-plateau-saddle-point"><strong>Problem of Plateau, Saddle point</strong></h4>
<p><em>Problem of Plateau是指目标函数在一个区域内非常平坦，梯度接近0。而saddle point指的是目标函数在某个区域梯度为0，但并非局部最小值（在某些维度上升，某些维度下降）。可以通过基于动量的优化器，以及自适应学习率解决这两个问题。</em></p>

<h4 id="when-transfer-learning-makes-sense"><strong>When transfer learning makes sense.</strong></h4>
<p><em>迁移学习是将在一个任务上学习到的知识（模型参数）应用到另一个任务上，当新的任务有监督数据较少，且和原任务有类似的原始输入特征时有效。如把基于ImageNet训练的CNN权重作为其它分类任务的
初始值，或者NLP领域的Pretrain + Finetune以及Prompt Tuning两个Paradigm，都能看成是迁移学习的成功应用。</em></p>]]></content><author><name></name></author><category term="Machine Learning" /><summary type="html"><![CDATA[试答机器学习理论知识中的一些常见问题 (1 / 3)]]></summary></entry><entry><title type="html">Fun Stuff - A problem with ants</title><link href="https://haoyuanpeng.github.io/blog/2019/ant/" rel="alternate" type="text/html" title="Fun Stuff - A problem with ants" /><published>2019-08-30T19:10:00+00:00</published><updated>2019-08-30T19:10:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2019/ant</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2019/ant/"><![CDATA[<p>本问题来自: <a href="http://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/puzzle/puzzle9.html">Puzzle toad</a></p>

<p>一个长度为60单位的圆周上随机分布着n只蚂蚁。在时间t=0时，每只蚂蚁各自独立地以0.5的概率顺时针爬行，以0.5的概率逆时针爬行，爬行的速度为每秒1单位。如果两只蚂蚁相撞，它们会瞬间各自掉头继续爬行。</p>

<p>对于一只特定的蚂蚁，求它在一分钟后，正好回到自己的出发点的概率。</p>]]></content><author><name></name></author><category term="fun stuff" /><category term="probability" /><summary type="html"><![CDATA[A Probability Problem About Ants]]></summary></entry><entry><title type="html">Fun Stuff - A puzzle of voting</title><link href="https://haoyuanpeng.github.io/blog/2019/vote/" rel="alternate" type="text/html" title="Fun Stuff - A puzzle of voting" /><published>2019-08-30T19:09:00+00:00</published><updated>2019-08-30T19:09:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2019/vote</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2019/vote/"><![CDATA[<p>本问题来自: <a href="http://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/puzzle/puzzle13.html">Puzzle toad</a></p>

<p>某个城市的选举通过一台电子投票机进行。有n位选民要对\(A、B、C\)三位候选人进行投票，第\(i\)位选民会从中选择一位作为自己的选项，记为\(x_i\)。
最终的投票结果由电子投票机根据各个选民的选票决定，记为\(f(x_1, x_2, ..., x_n)\)。</p>

<p>但是，函数\(f\)并不是简单地选取票数最多的候选人，其具体形式只有制造投票机的人才知道。制造投票机的人宣称能保证：如果每一位选民都改变了自己的选票，则选举结果一定会改变，因此这是一场公平的投票。</p>

<p>持怀疑观点的人认为，这场选举给人一种钦定的感觉，因为一定存在某位选民，使得最终选举结果仅和该选民的选择有关。他们的看法是正确的吗?</p>]]></content><author><name></name></author><category term="fun stuff" /><category term="puzzle" /><summary type="html"><![CDATA[A puzzle about voting]]></summary></entry><entry><title type="html">Fun Stuff - A puzzle about librarians</title><link href="https://haoyuanpeng.github.io/blog/2019/library/" rel="alternate" type="text/html" title="Fun Stuff - A puzzle about librarians" /><published>2019-08-06T20:01:00+00:00</published><updated>2019-08-06T20:01:00+00:00</updated><id>https://haoyuanpeng.github.io/blog/2019/library</id><content type="html" xml:base="https://haoyuanpeng.github.io/blog/2019/library/"><![CDATA[<p>本问题选自《浴缸里的惊叹》一书。</p>

<p>阿琨是一位图书管理员，他的职责是将书架上的10本书按顺序从左到右排列。但是，阿琨非常喜欢喝酒，由于酒后思考能力有限，他的操作只能是：每次拿起一本本应在更左边的书，将其插入它应该在的位置，而不管这一插入的操作是否改变了其它书的位置。请问，他是否必定能在有限步内，将所有的书按顺序排列？</p>

<p>阿鹤也是一位图书管理员，他比阿琨更喜欢喝酒。由于他喝的酒更多，他在酒后，只能拿起一本不在原位的书，将其插入它应该在的位置，而不管这一插入的操作是否改变了其它书的位置。请问，他是否必定能在有限步内，将所有的书按顺序排列？</p>]]></content><author><name></name></author><category term="fun stuff" /><category term="puzzle" /><summary type="html"><![CDATA[A proof question]]></summary></entry></feed>